# novaktj-transformer
**A decoder-only transformer, similar to GPT.**
I created a transformer model from scratch, using only PyTorch and a tokenizer. The code is a replication of 'Attention is all you need'.

I implemented:
1. Positional embedding
2. Multi-head attention
3. Training code

I also made unit tests and a toy task to quickly test model variations. 
